{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVID PREDICTION USING ML\n",
    "\n",
    "**Machine Learning**: Machine learning is the science of making computer learn and act like humans by feeding data and information without being explicitly programmed\n",
    "\n",
    "**Types of Machine Learning**: \n",
    "1. Supervised - Situations based on labeled data fed to the machine\n",
    "2. Unsupervised - Hidden pattern in an unlabeled data\n",
    "\n",
    "**Algorithms**:\n",
    "The method used to train the dataset and test it on testing dataset. eg. Linear regression, Random forest, Support vector machine(SVM) etc.\n",
    "\n",
    "In this project we aim to predict the spread of COVID-19 in India vs Asia using Linear regression, SVM and Artificial neural networks and compare the algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Importing important libraries***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Reading dataset***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '2021.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-8be651c08598>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcovid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'2021.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1048\u001b[0m             )\n\u001b[0;32m   1049\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1867\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m         \"\"\"\n\u001b[1;32m-> 1362\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '2021.csv'"
     ]
    }
   ],
   "source": [
    "covid=pd.read_csv('WHO.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Extracting India's dataset for COVID19***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "india_case=covid[covid[\"Country\"]==\"India\"].copy()\n",
    "index=np.arange(len(india_case))\n",
    "india_case=india_case.set_index(index)\n",
    "india_case.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Defining a function to plot learning curves***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "def plot_learning_curves(model, X, y):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=10)\n",
    "    train_errors, val_errors = [], []\n",
    "    for m in range(1, len(X_train)):\n",
    "        model.fit(X_train[:m], y_train[:m])\n",
    "        y_train_predict = model.predict(X_train[:m])\n",
    "        y_val_predict = model.predict(X_val)\n",
    "        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n",
    "        val_errors.append(mean_squared_error(y_val, y_val_predict))\n",
    "\n",
    "    plt.plot(np.sqrt(train_errors), color=\"#175F90\", linewidth=1.8, label=\"train\")\n",
    "    plt.plot(np.sqrt(val_errors), \"#5FBA83\",linewidth=1.8, label=\"val\")\n",
    "    plt.legend(loc=\"upper right\", fontsize=12)   \n",
    "    plt.xlabel(\"Training set size\", fontsize=12) \n",
    "    plt.ylabel(\"RMSE\", fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Defining a funtion to calculate Mean absolute percentage error***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAPE(y,y_pred):\n",
    "    MAPE = np.mean(np.abs(np.array(y) -\\\n",
    "                      np.array(y_pred))/np.array(y))\n",
    "    return MAPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## India COVID Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using seaborn and matplot library for data visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting confirmed cases per day vs date\n",
    "sns.set(rc={'figure.figsize':(15,10)})\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.lineplot(x=\"date\",y=\"total_cases\",data=india_case)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(15,10)})\n",
    "sns.lineplot(x=\"date\",y=\"Cumulative_deaths\",data=india_case)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need the time where COVID was at its peak so refining data to get the desired months where COVID was at peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "india_case2=india_case[(india_case[\"date\"]>=\"01-05-2020\") & (india_case[\"date\"]<\"30-09-2020\")]\n",
    "india_case2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using sklearn library for machine learning model. To build a model the date must be in **'date'** format and not in **'string'** format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply an algorithm a dataset must be divided into training and testing dataset. Using sklearn to split the data into 25% testing dataset and training dataset. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "warnings.filterwarnings('ignore')\n",
    "start=0\n",
    "dateHI=[]\n",
    "for i in range(0,len(india_case2.index)):\n",
    "    dateHI.append(start+i)\n",
    "india_case2['dates']=dateHI\n",
    "india_x=np.array(dateHI)\n",
    "india_y=india_case2['total_cases']\n",
    "india_z=india_case2['Cumulative_deaths']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Splitting data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "india_x_train,india_x_test,india_y_train,india_y_test=train_test_split(india_x,india_y,test_size=0.25)\n",
    "india_x_train2,india_x_test2,india_z_train,india_z_test=train_test_split(india_x,india_z,test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Training and predicting***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr=LinearRegression()\n",
    "import numpy as np\n",
    "lr.fit(np.array(india_x_train).reshape(-1,1),np.array(india_y_train).reshape(-1,1))\n",
    "lr_india_y_pred=lr.predict(np.array(india_x_test).reshape(-1,1))\n",
    "lr.fit(np.array(india_x_train2).reshape(-1,1),np.array(india_z_train).reshape(-1,1))\n",
    "lr_india_z_pred=lr.predict(np.array(india_x_test2).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Plot***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(1, 2)\n",
    "lr_india_y_pred=np.array(lr_india_y_pred).reshape(-1)\n",
    "lr_india_z_pred=np.array(lr_india_z_pred).reshape(-1)\n",
    "sns.scatterplot(india_x_test,india_y_test,color='#A7A7A7',s=75,legend=\"brief\",label=\"Actual\",ax=axes[0]).set_title('Total Cases per day')\n",
    "sns.lineplot(india_x_test,lr_india_y_pred,color=\"#A4D54E\",linewidth=2.25,label=\"Predicted\",ax=axes[0])\n",
    "sns.scatterplot(india_x_test2,india_z_test,color='#A7A7A7',s=75,legend=\"brief\",label=\"Actual\",ax=axes[1]).set_title('Cumulative deaths per day')\n",
    "sns.lineplot(india_x_test2,lr_india_z_pred,color=\"#F94350\",linewidth=2.25,label=\"Predicted\",ax=axes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Mean absolute Percentage Error***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MAPE of confirmed casses is \",MAPE(india_y_test,lr_india_y_pred)*100, \" %\")\n",
    "print(\"Accuracy for confirmed cases is: \",(1-MAPE(india_y_test,lr_india_y_pred))*100,\"\\n\")\n",
    "print(\"MAPE of deaths is \",MAPE(india_z_test,lr_india_z_pred)*100, \" %\")\n",
    "print(\"Accuracy for deaths is: \",(1-MAPE(india_z_test,lr_india_z_pred))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***R square***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that model is not good as the actual data seems to be an exponential and the fitting model is linear. Calculating R square tells how accurate a machine learning algorithm is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "score_lr_india1=r2_score(india_y_test, lr_india_y_pred)*100\n",
    "score_lr_india2=r2_score(india_z_test, lr_india_z_pred)*100\n",
    "print('R square score for total cases=',score_lr_india1)\n",
    "print('R square score for cumulative deaths=',score_lr_india2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Cross validation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "lr_scores_india1 = cross_val_score(lr, np.array(india_x_train).reshape(-1,1), np.array(india_y_train).reshape(-1,1), scoring='r2', cv=5)\n",
    "print(\"Cross Validation scores for total cases :\",lr_scores_india1)\n",
    "print(\"Mean of cross validation scores for total cases= \",np.mean(lr_scores_india1),'\\n')\n",
    "lr_scores_india2 = cross_val_score(lr, np.array(india_x_train2).reshape(-1,1), np.array(india_z_train).reshape(-1,1), scoring='r2', cv=5)\n",
    "print(\"Cross Validation scores for cumulative deaths :\",lr_scores_india2)\n",
    "print(\"Mean of cross validation scores for cumulative deaths= \",np.mean(lr_scores_india2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Learning Curve***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(lr,np.array(india_x_train).reshape(-1,1),np.array(india_y_train).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Adding polynomial feature***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly=PolynomialFeatures(degree=4)\n",
    "pr_x=poly.fit_transform(np.array(india_x_train).reshape(-1,1))\n",
    "pr_x2=poly.fit_transform(np.array(india_x_train2).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Training the model and predicting***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pr=LinearRegression()\n",
    "Pr.fit(pr_x,np.array(india_y_train).reshape(-1,1))\n",
    "pr_india1=Pr.predict(poly.fit_transform(np.array(india_x_test).reshape(-1,1)))\n",
    "pr_pred1=np.array(pr_india1).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pr.fit(pr_x2,np.array(india_z_train).reshape(-1,1))\n",
    "pr_india2=Pr.predict(poly.fit_transform(np.array(india_x_test2).reshape(-1,1)))\n",
    "pr_pred2=np.array(pr_india2).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Plot***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(1, 2)\n",
    "sns.scatterplot(india_x_test,india_y_test,color='#A7A7A7',s=75,legend=\"brief\",label=\"Actual\",ax=axes[0]).set_title('Total cases per day')\n",
    "sns.lineplot(india_x_test,y=pr_pred1,color=\"#A4D54E\",linewidth=2.25,label=\"Predicted\",ax=axes[0])\n",
    "sns.scatterplot(india_x_test2,india_z_test,color='#A7A7A7',s=75,legend=\"brief\",label=\"Actual\",ax=axes[1]).set_title('Cumulative deaths per day')\n",
    "sns.lineplot(india_x_test2,y=pr_pred2,color=\"#F94350\",linewidth=2.25,label=\"Predicted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Mean absolute percentage error***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MAPE of confirmed casses is \",MAPE(india_y_test,pr_pred1)*100, \" %\")\n",
    "print(\"Accuracy for confirmed cases is: \",(1-MAPE(india_y_test,pr_pred1))*100,\"\\n\")\n",
    "print(\"MAPE of deaths is \",MAPE(india_z_test,pr_pred2)*100, \" %\")\n",
    "print(\"Accuracy for deaths is: \",(1-MAPE(india_z_test,pr_pred2))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***R square***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_pr_india1=r2_score(india_y_test, pr_pred1)*100\n",
    "score_pr_india2=r2_score(india_z_test, pr_pred2)*100\n",
    "print('R square score for total cases=',score_pr_india1)\n",
    "print('R square score for cumulative deaths=',score_pr_india2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Cross Validation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_scores_india1 = cross_val_score(Pr, pr_x, np.array(india_y_train).reshape(-1,1), scoring='r2', cv=5)\n",
    "print(\"Cross Validation scores for total cases :\",pr_scores_india1)\n",
    "print(\"Mean of cross validation scores for total cases= \",np.mean(pr_scores_india1),'\\n')\n",
    "pr_scores_india2 = cross_val_score(Pr,pr_x2, np.array(india_z_train).reshape(-1,1), scoring='r2', cv=5)\n",
    "print(\"Cross Validation scores for cumulative deaths :\",pr_scores_india2)\n",
    "print(\"Mean of cross validation scores for cumulative deaths= \",np.mean(pr_scores_india2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Learning curve***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_learning_curves(Pr,pr_x,np.array(india_y_train).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Performing Augmented Dickey Fuller Test***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Performing ADF test\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from numpy import log\n",
    "adf_data=india_case2['total_cases']\n",
    "result = adfuller(adf_data)\n",
    "print('ADF Statistic: %f' % result[0])\n",
    "print('p-value: %f' % result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above results 'p' value is less than significance level(0.05) in order to reject null hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***To find 'd' value, plotting auto-correlation***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If more than 10 values are positive then it requires more differencing and it is non-stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(rc={'figure.figsize':(18,15)})\n",
    "# Original Series\n",
    "fig, axes = plt.subplots(4, 2)\n",
    "axes[0, 0].plot(adf_data); axes[0, 0].set_title('Original Series')\n",
    "plot_acf(adf_data, ax=axes[0, 1])\n",
    "\n",
    "# 1st Differencing\n",
    "axes[1, 0].plot(adf_data.diff()); axes[1, 0].set_title('1st Order Differencing')\n",
    "plot_acf(adf_data.diff().dropna(), ax=axes[1, 1])\n",
    "\n",
    "# 2nd Differencing\n",
    "axes[2, 0].plot(adf_data.diff().diff()); axes[2, 0].set_title('2nd Order Differencing')\n",
    "plot_acf(adf_data.diff().diff().dropna(), ax=axes[2, 1])\n",
    "\n",
    "#3rd Differencing\n",
    "axes[3, 0].plot(adf_data.diff().diff().diff()); axes[3, 0].set_title('3rd Order Differencing')\n",
    "plot_acf(adf_data.diff().diff().diff().dropna(), ax=axes[3, 1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above results, it can be observed that for order 1 differencing has more than 10 positive autocorrelation samples therefore it needs more differencing. Therefore d=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***To find 'p' value, plotting Partial auto correlation plot***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PACF plot of 1st differenced series\n",
    "sns.set(rc={'figure.figsize':(18,5)})\n",
    "\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "axes[0].plot(adf_data.diff().diff()); axes[0].set_title('2nd Differencing')\n",
    "axes[1].set(ylim=(0,5))\n",
    "plot_pacf(adf_data.diff().diff().dropna(), ax=axes[1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One sample is well above significance level therefore p=1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***To find 'q' value, plotting autocorrelation plot for order 2 differencing***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(18,5)})\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "axes[0].plot(adf_data.diff().diff()); axes[0].set_title('2nd Differencing')\n",
    "axes[1].set(ylim=(0,1.2))\n",
    "plot_acf(adf_data.diff().diff().dropna(), ax=axes[1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Training and forecasting using ARIMA***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "sns.set(rc={'figure.figsize':(15,10)})\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "#Splitting dataset\n",
    "split=int(len(india_case2)*0.8)\n",
    "arima_train=india_case2[:split]['total_cases']\n",
    "arima_test=india_case2[split:]['total_cases']\n",
    "arima_death_train=india_case2[:split]['Cumulative_deaths']\n",
    "arima_death_test=india_case2[split:]['Cumulative_deaths']\n",
    "\n",
    "# 1,2,2 ARIMA Model for confirmed cases\n",
    "model = ARIMA(arima_train, order=(1,2,2))\n",
    "confirmed = model.fit(disp=-1)\n",
    "# Forecast\n",
    "conf_fc, se, conf_ci = confirmed.forecast(len(india_case2)-split,alpha=0.05)  # 95% conf\n",
    "\n",
    "# Make as pandas series\n",
    "conf_fc = pd.Series(conf_fc, index=arima_test.index)\n",
    "conf_low = pd.Series(conf_ci[:, 0], index=arima_test.index)\n",
    "conf_high = pd.Series(conf_ci[:, 1], index=arima_test.index)\n",
    "\n",
    "# 1,2,2 ARIMA Model for deaths cases\n",
    "model = ARIMA(arima_death_train, order=(1,2,2))\n",
    "deaths = model.fit(disp=-1)\n",
    "# Forecast\n",
    "death_fc, se, death_ci = deaths.forecast(len(india_case2)-split,alpha=0.05)  # 95% conf\n",
    "\n",
    "# Make as pandas series\n",
    "death_fc = pd.Series(death_fc, index=arima_death_test.index)\n",
    "death_low = pd.Series(death_ci[:, 0], index=arima_death_test.index)\n",
    "death_high = pd.Series(death_ci[:, 1], index=arima_death_test.index)\n",
    "\n",
    "\n",
    "# Plot\n",
    "f, axes = plt.subplots(1, 2)\n",
    "axes[0].plot(arima_train, label='training')\n",
    "axes[0].plot(arima_test, label='actual')\n",
    "axes[0].plot(conf_fc, label='forecast')\n",
    "axes[0].fill_between(conf_low.index, conf_low, conf_high, \n",
    "                 color='k', alpha=.15)\n",
    "axes[0].set_title('Confirmed cases: actual vs forecast')\n",
    "axes[0].set_ylabel('Confirmed')\n",
    "axes[0].legend(loc='upper left', fontsize=8)\n",
    "axes[1].plot(arima_death_train, label='training')\n",
    "axes[1].plot(arima_death_test, label='actual')\n",
    "axes[1].plot(death_fc, label='forecast')\n",
    "axes[1].fill_between(death_low.index, death_low, death_high, \n",
    "                 color='k', alpha=.15)\n",
    "axes[1].set_ylabel('Deaths')\n",
    "axes[1].set_title('Deaths: actual vs forecast')\n",
    "axes[1].legend(loc='upper left', fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Mean absolute percentage error***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MAPE of confirmed casses is \",MAPE(arima_test,conf_fc)*100, \" %\")\n",
    "print(\"Accuracy for confirmed cases is: \",(1-MAPE(arima_test,conf_fc))*100,\"\\n\")\n",
    "print(\"MAPE of deaths is \",MAPE(arima_death_test,death_fc)*100, \" %\")\n",
    "print(\"Accuracy for deaths is: \",(1-MAPE(arima_death_test,death_fc))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron using keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Scaling data and splitting into training and testing***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For confirmed cases\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#Take 10 steps to predict the 11th data point\n",
    "n_steps = 10\n",
    "n_features = 1 \n",
    "#Splitting training and testing data\n",
    "size_x_Train=len(india_x)-n_steps\n",
    "mlp_india_y=np.array(india_y).reshape(-1,1)\n",
    "mlp_train=mlp_india_y[:size_x_Train]\n",
    "mlp_test=mlp_india_y[size_x_Train:]\n",
    "#Scaling data\n",
    "mlp_scaler=MinMaxScaler()\n",
    "mlp_scaler=mlp_scaler.fit(mlp_train)\n",
    "mlp_train=mlp_scaler.transform(mlp_train)\n",
    "mlp_test=mlp_scaler.transform(mlp_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Using TimeseriesGenerator***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "n_input=n_steps\n",
    "generator = TimeseriesGenerator(mlp_train,mlp_train,length = n_input,batch_size=1)\n",
    "print(len(generator))\n",
    "for i in range(len(generator)-2,len(generator)):\n",
    "    x, y = generator[i]\n",
    "    print('%s => %s' % (x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Defining neural network***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation,Flatten\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(192, activation='relu',input_shape=(n_input,n_features)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=1))\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Preparing a validation set***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set = np.append(mlp_train[-1],mlp_test)\n",
    "val_set=val_set.reshape(n_steps+1,1)\n",
    "val_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = n_steps\n",
    "n_features = 1\n",
    "validation_gen = TimeseriesGenerator(val_set,val_set,length = n_input,batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stop = EarlyStopping(monitor='val_loss',patience=25,restore_best_weights=True)\n",
    "\n",
    "# fit the model\n",
    "model.fit_generator(generator,validation_data=validation_gen,epochs=200,callbacks=[early_stop],steps_per_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(model.history.history).plot(title=\"Loss vs epochs curve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Taking test batch as last 10 data points***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of predictions\n",
    "mlp_pred = []\n",
    "\n",
    "# last `n_input` points from training set\n",
    "test_batch = mlp_train[-n_input:].reshape(1,n_input,n_features)\n",
    "\n",
    "test_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Forecasting for 50 unseen observations***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# forecast the number of confirmed cases in India for the validation set and the next 50 days\n",
    "\n",
    "for i in range(n_steps+50):\n",
    "    batch_pred = model.predict(test_batch)[0]\n",
    "    mlp_pred.append(batch_pred)\n",
    "    test_batch = np.append(test_batch[:,1:,:],[[batch_pred]],axis=1)\n",
    "\n",
    "mlp_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply inverse transformations on scaled data\n",
    "mlp_pred = mlp_scaler.inverse_transform(mlp_pred)\n",
    "mlp_pred[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_day=size_x_Train\n",
    "future=india_x[last_day:]\n",
    "for i in range(len(india_x),len(india_x)+50):\n",
    "    future=np.append(future,i)\n",
    "print(future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from itertools import repeat\n",
    "mlp_conf_df = pd.DataFrame(columns=[\"Confirmed\",\"Confirmed_predicted\"],index=future)\n",
    "mlp_conf_df.loc[:,\"Confirmed_predicted\"] = mlp_pred[:,0]\n",
    "test_set=india_case2.iloc[size_x_Train:]['total_cases'].values.tolist()\n",
    "nan_list=[]\n",
    "nan_list=nan_list.extend(repeat(None,50))\n",
    "mlp_conf_df.loc[:,\"Confirmed\"] =india_case2.iloc[size_x_Train:]['total_cases'].values.tolist()+[None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***For Death cases, Similarly...***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For death cases\n",
    "\n",
    "mlp_deaths_india=np.array(india_z).reshape(-1,1)\n",
    "mlp_deaths_train=mlp_deaths_india[:size_x_Train]\n",
    "mlp_deaths_test=mlp_deaths_india[size_x_Train:]\n",
    "#Scaling data\n",
    "mlp_scaler2=MinMaxScaler()\n",
    "mlp_scaler2=mlp_scaler2.fit(mlp_deaths_train)\n",
    "mlp_deaths_train=mlp_scaler2.transform(mlp_deaths_train)\n",
    "mlp_deaths_test=mlp_scaler2.transform(mlp_deaths_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "n_input=n_steps\n",
    "generator = TimeseriesGenerator(mlp_deaths_train,mlp_deaths_train,length = n_input,batch_size=1)\n",
    "for i in range(len(generator)-2,len(generator)):\n",
    "    x, y = generator[i]\n",
    "    print('%s => %s' % (x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set = np.append(mlp_deaths_train[-1],mlp_deaths_test)\n",
    "val_set=val_set.reshape(n_steps+1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = n_steps\n",
    "n_features = 1\n",
    "validation_gen = TimeseriesGenerator(val_set,val_set,length = n_input,batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stop = EarlyStopping(monitor='val_loss',patience=25,restore_best_weights=True)\n",
    "\n",
    "# fit the model\n",
    "model.fit_generator(generator,validation_data=validation_gen,epochs=200,callbacks=[early_stop],steps_per_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of predictions\n",
    "mlp_pred2 = []\n",
    "\n",
    "# last `n_input` points from training set\n",
    "test_batch = mlp_deaths_train[-n_input:].reshape(1,n_input,n_features)\n",
    "\n",
    "test_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# forecast the number of deaths in India for the validation set and the next 50 days\n",
    "\n",
    "for i in range(n_steps+50):\n",
    "    batch_pred = model.predict(test_batch)[0]\n",
    "    mlp_pred2.append(batch_pred)\n",
    "    test_batch = np.append(test_batch[:,1:,:],[[batch_pred]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply inverse transformations on scaled data\n",
    "mlp_pred2 = mlp_scaler2.inverse_transform(mlp_pred2)\n",
    "mlp_pred2[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import repeat\n",
    "mlp_deaths_df = pd.DataFrame(columns=[\"Deaths\",\"Deaths_predicted\"],index=future)\n",
    "mlp_deaths_df.loc[:,\"Deaths_predicted\"] = mlp_pred2[:,0]\n",
    "test_set=india_case2.iloc[size_x_Train:]['Cumulative_deaths'].values.tolist()\n",
    "nan_list=[]\n",
    "nan_list=nan_list.extend(repeat(None,50))\n",
    "mlp_deaths_df.loc[:,\"Deaths\"] =india_case2.iloc[size_x_Train:]['Cumulative_deaths'].values.tolist()+[None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(1, 2)\n",
    "mlp_conf_df.plot(title=\"Confirmed Predictions for next 50 days-MLP\",ax=axes[0])\n",
    "mlp_deaths_df.plot(title=\"Death Predictions for next 50 days-MLP\",ax=axes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import repeat\n",
    "present=future\n",
    "present=np.append(np.arange(future[0]),future)\n",
    "mlp = pd.DataFrame(columns=[\"Confirmed\",\"Confirmed_predicted,Deaths,Deaths_predicted\"],index=present)\n",
    "mlp.loc[:future[0],\"Confirmed\"] = india_case2.iloc[:size_x_Train+1]['total_cases'].values.tolist()\n",
    "mlp.loc[:future[0],\"Deaths\"] = india_case2.iloc[:size_x_Train+1]['Cumulative_deaths'].values.tolist()\n",
    "nan_list=[]\n",
    "nan_list.extend(repeat(None,167))\n",
    "mlp.loc[:,\"Confirmed_predicted\"] =nan_list+ mlp_pred[:,0].tolist()\n",
    "mlp.loc[:,\"Deaths_predicted\"] =nan_list+ mlp_pred2[:,0].tolist()\n",
    "mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2)\n",
    "mlp.plot(y='Confirmed',ax=axes[0],linewidth=2).set_title('Time Forecasting')\n",
    "mlp.plot(y='Confirmed_predicted',linestyle='dashed',ax=axes[0],linewidth=2)\n",
    "mlp.plot(y='Deaths',ax=axes[1],linewidth=2).set_title('Time Forecasting')\n",
    "mlp.plot(y='Deaths_predicted',linestyle='dashed',ax=axes[1],linewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MAPE of confirmed casses is \",MAPE(mlp_conf_df[\"Confirmed\"][:n_steps],mlp_conf_df[\"Confirmed_predicted\"][:n_steps])*100,\" %\")\n",
    "print(\"Accuracy for confirmed cases is: \",(1-MAPE(mlp_conf_df[\"Confirmed\"][:n_steps],mlp_conf_df[\"Confirmed_predicted\"][:n_steps]))*100,\"\\n\")\n",
    "print(\"MAPE of deaths is \",MAPE(mlp_deaths_df[\"Deaths\"][:n_steps],mlp_deaths_df[\"Deaths_predicted\"][:n_steps]), \" %\")\n",
    "print(\"Accuracy for deaths is: \",(1-MAPE(mlp_deaths_df[\"Deaths\"][:n_steps],mlp_deaths_df[\"Deaths_predicted\"][:n_steps]))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Importing the SVR model from sklearn SVM module and training the model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scalerX1 = MinMaxScaler()\n",
    "scalerY1 = MinMaxScaler()\n",
    "scalerX2 = MinMaxScaler()\n",
    "scalerY2 = MinMaxScaler()\n",
    "svr_x_train1 = scalerX1.fit_transform(np.array(india_x_train).reshape(-1,1))\n",
    "svr_y_train1 = scalerY1.fit_transform(np.array(india_y_train).reshape(-1,1))\n",
    "svr_x_test1 = scalerX1.transform(np.array(india_x_test).reshape(-1,1))\n",
    "svr_y_test1 = scalerY1.transform(np.array(india_y_test).reshape(-1,1))\n",
    "svr_x_train2 = scalerX2.fit_transform(np.array(india_x_train2).reshape(-1,1))\n",
    "svr_z_train = scalerY2.fit_transform(np.array(india_z_train).reshape(-1,1))\n",
    "svr_x_test2 = scalerX2.transform(np.array(india_x_test2).reshape(-1,1))\n",
    "svr_z_test = scalerY2.transform(np.array(india_z_test).reshape(-1,1))\n",
    "svm=SVR(kernel=\"rbf\", C=1,epsilon=0.01,gamma='scale')\n",
    "svm.fit(svr_x_train1,svr_y_train1)\n",
    "svr_india_pred1=scalerY1.inverse_transform(np.array(svm.predict(svr_x_test1)).reshape(-1,1))\n",
    "svr_india_pred1=np.array(svr_india_pred1).reshape(-1)\n",
    "svm.fit(svr_x_train2,svr_z_train)\n",
    "svr_india_pred2=scalerY2.inverse_transform(np.array(svm.predict(svr_x_test2)).reshape(-1,1))\n",
    "svr_india_pred2=np.array(svr_india_pred2).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Plotting model vs actual***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(1, 2)\n",
    "sns.scatterplot(india_x_test,india_y_test,color='#A7A7A7',s=75,label=\"Actual\",ax=axes[0])\n",
    "sns.lineplot(india_x_test,np.array(svr_india_pred1).reshape(-1),color=\"#A4D54E\",linewidth=2.25,label=\"Predicted\",ax=axes[0])\n",
    "sns.scatterplot(india_x_test2,india_z_test,color='#A7A7A7',s=75,label=\"Actual\",ax=axes[1])\n",
    "sns.lineplot(india_x_test2,np.array(svr_india_pred2).reshape(-1),color=\"#F94350\",linewidth=2.25,label=\"Predicted\",ax=axes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***R square***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"R square score for total cases\",r2_score(india_y_test, svr_india_pred1)*100)\n",
    "print(\"R square score for total cases\",r2_score(india_z_test, svr_india_pred2)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Cross Validation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_scores1 = cross_val_score(svm, np.array(svr_x_train1).reshape(-1,1), np.array(svr_y_train1).reshape(-1,1), scoring='r2', cv=5)\n",
    "print(\"Cross Validation scores for total cases :\",svr_scores1)\n",
    "print(\"Mean of cross validation scores for total cases= \",np.mean(svr_scores1),'\\n')\n",
    "svr_scores2 = cross_val_score(svm, np.array(svr_x_train1).reshape(-1,1), np.array(svr_y_train1).reshape(-1,1), scoring='r2', cv=5)\n",
    "print(\"Cross Validation scores for cumulative deaths :\",svr_scores2)\n",
    "print(\"Mean of cross validation scores for cumulative deaths= \",np.mean(svr_scores2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Learning curve***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(svm,np.array(svr_x_train1).reshape(-1,1),np.array(svr_y_train1).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Mean absolute percentage error***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MAPE of confirmed casses is \",MAPE(india_y_test,svr_india_pred1)*100, \" %\")\n",
    "print(\"Accuracy for confirmed cases is: \",(1-MAPE(india_y_test,svr_india_pred1))*100,\"\\n\")\n",
    "print(\"MAPE of deaths is \",MAPE(india_z_test,svr_india_pred2)*100, \" %\")\n",
    "print(\"Accuracy for deaths is: \",(1-MAPE(india_z_test,svr_india_pred2))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asian countries(except India) Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asia=['China','India','Hong Kong','Indonesia','Pakistan','Japan','Philippines','Vietnam','Turkey','Iran','Thailand','Myanmar','South Korea','Iraq','Afghanistan','Saudi Arabia','Malaysia','Yemen','Nepal','Sri lanka','Kazakhstan','Syria','Jordan','Israel','Singapore','Laos','Lebanon','Oman','United Arab Emirates','Kuwait','Georgia','Mangolia','Armenia','Qatar','Russia','Bahrain','Timor-Leste','Cyprus','Bhutan','Maldives','Brunei','Taiwan']\n",
    "asia_case=covid[covid['Country'].isin(asia)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select all countries except India \n",
    "except_india_asia=asia_case[asia_case['Country']!='India']\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime as dt\n",
    "#except_india_asia['date']=pd.to_datetime(except_india_asia['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#except_india_asia['date']=except_india_asia['date'].map(dt.datetime.toordinal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset contains all the continents and each continent has many countries. We require a data such that for each date the total number of cases by continent are known. Using pandas_df.groupby([]) helps us achieve this. It is similar to GROUP BY in SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data=except_india_asia.groupby(['date'],as_index=False,sort=False).sum()\n",
    "grouped_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Data Visualisation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting confirmed cases per day vs date\n",
    "sns.lineplot(x=\"date\",y=\"total_cases\",data=grouped_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting confirmed cases per day vs date\n",
    "sns.set(rc={'figure.figsize':(15,10)})\n",
    "sns.lineplot(x=\"date\",y=\"Cumulative_deaths\",data=grouped_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=0\n",
    "dateHI2=[]\n",
    "for i in range(0,len(grouped_data.index)):\n",
    "    dateHI2.append(start+i)\n",
    "grouped_data['datess']=dateHI2\n",
    "x=np.array(dateHI2)\n",
    "asiaE_x=x\n",
    "asiaE_y=grouped_data['total_cases']\n",
    "asiaE_z=grouped_data['Cumulative_deaths']\n",
    "asiaE_x_train,asiaE_x_test,asiaE_y_train,asiaE_y_test=train_test_split(asiaE_x,asiaE_y,test_size=0.25)\n",
    "asiaE_x_train2,asiaE_x_test2,asiaE_z_train,asiaE_z_test=train_test_split(asiaE_x,asiaE_z,test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Fitting the linear model on training dataset and predicting***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=LinearRegression()\n",
    "lr.fit(np.array(asiaE_x_train).reshape(-1,1),np.array(asiaE_y_train).reshape(-1,1))\n",
    "lr_asiaE_pred1=lr.predict(np.array(asiaE_x_test).reshape(-1,1))\n",
    "lr.fit(np.array(asiaE_x_train2).reshape(-1,1),np.array(asiaE_z_train).reshape(-1,1))\n",
    "lr_asiaE_pred2=lr.predict(np.array(asiaE_x_test2).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Plot - Asia(except India)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_asiaE_pred1=np.array(lr_asiaE_pred1).reshape(-1)\n",
    "lr_asiaE_pred2=np.array(lr_asiaE_pred2).reshape(-1)\n",
    "f, axes = plt.subplots(1, 2)\n",
    "sns.scatterplot(asiaE_x_test,asiaE_y_test,color='#A7A7A7',s=75,legend=\"brief\",label=\"Actual\",ax=axes[0])\n",
    "sns.lineplot(asiaE_x_test,lr_asiaE_pred1,color=\"#A4D54E\",linewidth=2.25,label=\"Predicted\",ax=axes[0])\n",
    "sns.scatterplot(asiaE_x_test2,asiaE_z_test,color='#A7A7A7',s=75,legend=\"brief\",label=\"Actual\")\n",
    "sns.lineplot(asiaE_x_test2,lr_asiaE_pred2,color=\"#F94350\",linewidth=2.25,label=\"Predicted\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Mean absolute percentage error***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MAPE of confirmed casses is \",MAPE(asiaE_y_test,lr_asiaE_pred1)*100, \" %\")\n",
    "print(\"Accuracy for confirmed cases is: \",(1-MAPE(asiaE_y_test,lr_asiaE_pred1))*100,\"\\n\")\n",
    "print(\"MAPE of deaths is \",MAPE(asiaE_z_test,lr_asiaE_pred2)*100, \" %\")\n",
    "print(\"Accuracy for deaths is: \",(1-MAPE(asiaE_z_test,lr_asiaE_pred2))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Calculating R square***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_pr_india1=r2_score(asiaE_y_test, lr_asiaE_pred1)*100\n",
    "score_pr_india2=r2_score(asiaE_z_test, lr_asiaE_pred2)*100\n",
    "print('R square score for total cases=',score_pr_india1)\n",
    "print('R square score for cumulative deaths=',score_pr_india2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Cross Validation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scores_asiaE1 = cross_val_score(lr, np.array(asiaE_x_train).reshape(-1,1), np.array(asiaE_y_train).reshape(-1,1), scoring='r2', cv=5)\n",
    "print(\"Cross Validation scores for total cases :\",lr_scores_asiaE1)\n",
    "print(\"Mean of cross validation scores for total cases= \",np.mean(lr_scores_asiaE1),'\\n')\n",
    "lr_scores_asiaE2 = cross_val_score(lr, np.array(asiaE_x_train2).reshape(-1,1), np.array(asiaE_z_train).reshape(-1,1), scoring='r2', cv=5)\n",
    "print(\"Cross Validation scores for cumulative deaths :\",lr_scores_asiaE2)\n",
    "print(\"Mean of cross validation scores for cumulative deaths= \",np.mean(lr_scores_asiaE2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Learning curve***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(lr,np.array(asiaE_x_train).reshape(-1,1),np.array(asiaE_y_train).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Adding extra feature***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly2=PolynomialFeatures(degree=2)\n",
    "pr_x_asia=poly.fit_transform(np.array(asiaE_x_train).reshape(-1,1))\n",
    "pr_x_asia2=poly.fit_transform(np.array(asiaE_x_train2).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Training the model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pr=LinearRegression()\n",
    "Pr.fit(pr_x_asia,np.array(asiaE_y_train).reshape(-1,1))\n",
    "pr_asiaE_pred1=np.array(Pr.predict(poly.fit_transform(np.array(asiaE_x_test).reshape(-1,1)))).reshape(-1)\n",
    "Pr.fit(pr_x_asia2,np.array(asiaE_z_train).reshape(-1,1))\n",
    "pr_asiaE_pred2=np.array(Pr.predict(poly.fit_transform(np.array(asiaE_x_test2).reshape(-1,1)))).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Plot***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(1, 2)\n",
    "sns.scatterplot(asiaE_x_test,asiaE_y_test,color='#A7A7A7',s=75,legend=\"brief\",label=\"Actual\",ax=axes[0])\n",
    "sns.lineplot(asiaE_x_test,pr_asiaE_pred1,color=\"#A4D54E\",linewidth=2.25,label=\"Predicted\",ax=axes[0])\n",
    "sns.scatterplot(asiaE_x_test2,asiaE_z_test,color='#A7A7A7',s=75,legend=\"brief\",label=\"Actual\",ax=axes[1])\n",
    "sns.lineplot(asiaE_x_test2,pr_asiaE_pred2,color=\"#F94350\",linewidth=2.25,label=\"Predicted\",ax=axes[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Mean absolute percentage error***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MAPE of confirmed casses is \",MAPE(asiaE_y_test,pr_asiaE_pred1)*100, \" %\")\n",
    "print(\"Accuracy for confirmed cases is: \",(1-MAPE(asiaE_y_test,pr_asiaE_pred1))*100,\"\\n\")\n",
    "print(\"MAPE of deaths is \",MAPE(asiaE_z_test,pr_asiaE_pred2)*100, \" %\")\n",
    "print(\"Accuracy for deaths is: \",(1-MAPE(asiaE_z_test,pr_asiaE_pred2))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***R square***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_pr_india1=r2_score(asiaE_y_test, pr_asiaE_pred1)*100\n",
    "score_pr_india2=r2_score(asiaE_z_test, pr_asiaE_pred2)*100\n",
    "print('R square score for total cases=',score_pr_india1)\n",
    "print('R square score for cumulative deaths=',score_pr_india2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Cross Validation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_scores_asiaE1 = cross_val_score(Pr, pr_x_asia, np.array(asiaE_y_train).reshape(-1,1), scoring='r2', cv=5)\n",
    "print(\"Cross Validation scores for total case \",pr_scores_asiaE1)\n",
    "print(\"The mean score after cross validation for total cases is\",np.mean(pr_scores_asiaE1),\"\\n\")\n",
    "pr_scores_asiaE2 = cross_val_score(Pr, pr_x_asia2, np.array(asiaE_z_train).reshape(-1,1), scoring='r2', cv=5)\n",
    "print(\"Cross Validation scores for total case \",pr_scores_asiaE2)\n",
    "print(\"The mean score after cross validation for total cases is\",np.mean(pr_scores_asiaE2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Learning Curve***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(Pr,pr_x_asia,np.array(asiaE_y_train).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMA Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing ADF test\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from numpy import log\n",
    "adf_data=asiaE_y\n",
    "result = adfuller(adf_data)\n",
    "print('ADF Statistic: %f' % result[0])\n",
    "print('p-value: %f' % result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(rc={'figure.figsize':(18,15)})\n",
    "# Original Series\n",
    "fig, axes = plt.subplots(4, 2)\n",
    "axes[0, 0].plot(adf_data); axes[0, 0].set_title('Original Series')\n",
    "plot_acf(adf_data, ax=axes[0, 1])\n",
    "\n",
    "# 1st Differencing\n",
    "axes[1, 0].plot(adf_data.diff()); axes[1, 0].set_title('1st Order Differencing')\n",
    "plot_acf(adf_data.diff().dropna(), ax=axes[1, 1])\n",
    "\n",
    "# 2nd Differencing\n",
    "axes[2, 0].plot(adf_data.diff().diff()); axes[2, 0].set_title('2nd Order Differencing')\n",
    "plot_acf(adf_data.diff().diff().dropna(), ax=axes[2, 1])\n",
    "\n",
    "#3rd Differencing\n",
    "axes[3, 0].plot(adf_data.diff().diff().diff()); axes[3, 0].set_title('3rd Order Differencing')\n",
    "plot_acf(adf_data.diff().diff().diff().dropna(), ax=axes[3, 1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PACF plot of 2nd differenced series\n",
    "sns.set(rc={'figure.figsize':(18,5)})\n",
    "\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "axes[0].plot(adf_data.diff().diff()); axes[0].set_title('2nd Differencing')\n",
    "axes[1].set(ylim=(0,5))\n",
    "plot_pacf(adf_data.diff().diff().dropna(), ax=axes[1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'figure.figsize':(18,5), 'figure.dpi':100})\n",
    "\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "axes[0].plot(adf_data.diff().diff()); axes[0].set_title('2nd Differencing')\n",
    "axes[1].set(ylim=(0,1.2))\n",
    "plot_acf(adf_data.diff().diff().dropna(), ax=axes[1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "sns.set(rc={'figure.figsize':(15,10)})\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "#Splitting dataset\n",
    "split=int(len(grouped_data)*0.8)\n",
    "arima_train=grouped_data[:split]['total_cases']\n",
    "arima_test=grouped_data[split:]['total_cases']\n",
    "arima_death_train=grouped_data[:split]['Cumulative_deaths']\n",
    "arima_death_test=grouped_data[split:]['Cumulative_deaths']\n",
    "\n",
    "# 1,2,2 ARIMA Model for confirmed cases\n",
    "model = ARIMA(arima_train, order=(1,2,2))\n",
    "confirmed = model.fit(disp=-1)\n",
    "# Forecast\n",
    "conf_fc, se, conf_ci = confirmed.forecast(len(grouped_data)-split,alpha=0.05)  # 95% conf\n",
    "\n",
    "# Make as pandas series\n",
    "conf_fc = pd.Series(conf_fc, index=arima_test.index)\n",
    "conf_low = pd.Series(conf_ci[:, 0], index=arima_test.index)\n",
    "conf_high = pd.Series(conf_ci[:, 1], index=arima_test.index)\n",
    "\n",
    "# 1,2,2 ARIMA Model for deaths cases\n",
    "model = ARIMA(arima_death_train, order=(1,1,1))\n",
    "deaths = model.fit(disp=-1)\n",
    "# Forecast\n",
    "death_fc, se, death_ci = deaths.forecast(len(grouped_data)-split,alpha=0.05)  # 95% conf\n",
    "\n",
    "# Make as pandas series\n",
    "death_fc = pd.Series(death_fc, index=arima_death_test.index)\n",
    "death_low = pd.Series(death_ci[:, 0], index=arima_death_test.index)\n",
    "death_high = pd.Series(death_ci[:, 1], index=arima_death_test.index)\n",
    "\n",
    "\n",
    "# Plot\n",
    "f, axes = plt.subplots(1, 2)\n",
    "axes[0].plot(arima_train, label='training')\n",
    "axes[0].plot(arima_test, label='actual')\n",
    "axes[0].plot(conf_fc, label='forecast')\n",
    "axes[0].fill_between(conf_low.index, conf_low, conf_high, \n",
    "                 color='k', alpha=.15)\n",
    "axes[0].set_title('Confirmed cases: actual vs forecast')\n",
    "axes[0].set_ylabel('Confirmed')\n",
    "axes[0].legend(loc='upper left', fontsize=8)\n",
    "axes[1].plot(arima_death_train, label='training')\n",
    "axes[1].plot(arima_death_test, label='actual')\n",
    "axes[1].plot(death_fc, label='forecast')\n",
    "axes[1].fill_between(death_low.index, death_low, death_high, \n",
    "                 color='k', alpha=.15)\n",
    "axes[1].set_ylabel('Deaths')\n",
    "axes[1].set_title('Deaths: actual vs forecast')\n",
    "axes[1].legend(loc='upper left', fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MAPE of confirmed casses is \",MAPE(arima_test,conf_fc)*100, \" %\")\n",
    "print(\"Accuracy for confirmed cases is: \",(1-MAPE(arima_test,conf_fc))*100,\"\\n\")\n",
    "print(\"MAPE of deaths is \",MAPE(arima_death_test,death_fc)*100, \" %\")\n",
    "print(\"Accuracy for deaths is: \",(1-MAPE(arima_death_test,death_fc))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For confirmed cases\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#Take 10 steps to predict the 11th data point\n",
    "n_steps = 10\n",
    "n_features = 1 \n",
    "#Splitting training and testing data\n",
    "size_x_Train=len(asiaE_x)-n_steps\n",
    "mlp_asia_y=np.array(asiaE_y).reshape(-1,1)\n",
    "mlp_train=mlp_asia_y[:size_x_Train]\n",
    "mlp_test=mlp_asia_y[size_x_Train:]\n",
    "#Scaling data\n",
    "mlp_scaler=MinMaxScaler()\n",
    "mlp_scaler=mlp_scaler.fit(mlp_train)\n",
    "mlp_train=mlp_scaler.transform(mlp_train)\n",
    "mlp_test=mlp_scaler.transform(mlp_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "n_input=n_steps\n",
    "generator = TimeseriesGenerator(mlp_train,mlp_train,length = n_input,batch_size=1)\n",
    "print(len(generator))\n",
    "for i in range(len(generator)-2,len(generator)):\n",
    "    x, y = generator[i]\n",
    "    print('%s => %s' % (x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation,Flatten\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(192, activation='relu',input_shape=(n_input,n_features)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=1))\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set = np.append(mlp_train[-1],mlp_test)\n",
    "val_set=val_set.reshape(n_steps+1,1)\n",
    "val_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = n_steps\n",
    "n_features = 1\n",
    "validation_gen = TimeseriesGenerator(val_set,val_set,length = n_input,batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stop = EarlyStopping(monitor='val_loss',patience=25,restore_best_weights=True)\n",
    "\n",
    "# fit the model\n",
    "model.fit_generator(generator,validation_data=validation_gen,epochs=200,callbacks=[early_stop],steps_per_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(model.history.history).plot(title=\"Loss vs epochs curve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of predictions\n",
    "mlp_pred = []\n",
    "\n",
    "# last `n_input` points from training set\n",
    "test_batch = mlp_train[-n_input:].reshape(1,n_input,n_features)\n",
    "\n",
    "test_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecast the number of confirmed cases in India for the validation set and the next 50 days\n",
    "\n",
    "for i in range(n_steps+50):\n",
    "    batch_pred = model.predict(test_batch)[0]\n",
    "    mlp_pred.append(batch_pred)\n",
    "    test_batch = np.append(test_batch[:,1:,:],[[batch_pred]],axis=1)\n",
    "\n",
    "mlp_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply inverse transformations on scaled data\n",
    "mlp_pred = mlp_scaler.inverse_transform(mlp_pred)\n",
    "mlp_pred[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_day=size_x_Train\n",
    "future=asiaE_x[last_day:]\n",
    "for i in range(len(asiaE_x),len(asiaE_x)+50):\n",
    "    future=np.append(future,i)\n",
    "print(future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import repeat\n",
    "mlp_conf_df = pd.DataFrame(columns=[\"Confirmed\",\"Confirmed_predicted\"],index=future)\n",
    "mlp_conf_df.loc[:,\"Confirmed_predicted\"] = mlp_pred[:,0]\n",
    "test_set=grouped_data.iloc[size_x_Train:]['total_cases'].values.tolist()\n",
    "nan_list=[]\n",
    "nan_list=nan_list.extend(repeat(None,50))\n",
    "mlp_conf_df.loc[:,\"Confirmed\"] =grouped_data.iloc[size_x_Train:]['total_cases'].values.tolist()+[None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For death cases\n",
    "\n",
    "mlp_deaths_asia=np.array(asiaE_z).reshape(-1,1)\n",
    "mlp_deaths_train=mlp_deaths_asia[:size_x_Train]\n",
    "mlp_deaths_test=mlp_deaths_asia[size_x_Train:]\n",
    "#Scaling data\n",
    "mlp_scaler2=MinMaxScaler()\n",
    "mlp_scaler2=mlp_scaler2.fit(mlp_deaths_train)\n",
    "mlp_deaths_train=mlp_scaler2.transform(mlp_deaths_train)\n",
    "mlp_deaths_test=mlp_scaler2.transform(mlp_deaths_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "n_input=n_steps\n",
    "generator = TimeseriesGenerator(mlp_deaths_train,mlp_deaths_train,length = n_input,batch_size=1)\n",
    "for i in range(len(generator)-2,len(generator)):\n",
    "    x, y = generator[i]\n",
    "    print('%s => %s' % (x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set = np.append(mlp_deaths_train[-1],mlp_deaths_test)\n",
    "val_set=val_set.reshape(n_steps+1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = n_steps\n",
    "n_features = 1\n",
    "validation_gen = TimeseriesGenerator(val_set,val_set,length = n_input,batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stop = EarlyStopping(monitor='val_loss',patience=25,restore_best_weights=True)\n",
    "\n",
    "# fit the model\n",
    "model.fit_generator(generator,validation_data=validation_gen,epochs=200,callbacks=[early_stop],steps_per_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of predictions\n",
    "mlp_pred2 = []\n",
    "\n",
    "# last `n_input` points from training set\n",
    "test_batch = mlp_deaths_train[-n_input:].reshape(1,n_input,n_features)\n",
    "\n",
    "test_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# forecast the number of deaths in India for the validation set and the next 50 days\n",
    "\n",
    "for i in range(n_steps+50):\n",
    "    batch_pred = model.predict(test_batch)[0]\n",
    "    mlp_pred2.append(batch_pred)\n",
    "    test_batch = np.append(test_batch[:,1:,:],[[batch_pred]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply inverse transformations on scaled data\n",
    "mlp_pred2 = mlp_scaler2.inverse_transform(mlp_pred2)\n",
    "mlp_pred2[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import repeat\n",
    "mlp_deaths_df = pd.DataFrame(columns=[\"Deaths\",\"Deaths_predicted\"],index=future)\n",
    "mlp_deaths_df.loc[:,\"Deaths_predicted\"] = mlp_pred2[:,0]\n",
    "test_set=grouped_data.iloc[size_x_Train:]['Cumulative_deaths'].values.tolist()\n",
    "nan_list=[]\n",
    "nan_list=nan_list.extend(repeat(None,50))\n",
    "mlp_deaths_df.loc[:,\"Deaths\"] =grouped_data.iloc[size_x_Train:]['Cumulative_deaths'].values.tolist()+[None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(1, 2)\n",
    "mlp_conf_df.plot(title=\"Confirmed Predictions for next 50 days-MLP\",ax=axes[0])\n",
    "mlp_deaths_df.plot(title=\"Death Predictions for next 50 days-MLP\",ax=axes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import repeat\n",
    "present=future\n",
    "present=np.append(np.arange(future[0]),future)\n",
    "mlp = pd.DataFrame(columns=[\"Confirmed\",\"Confirmed_predicted,Deaths,Deaths_predicted\"],index=present)\n",
    "mlp.loc[:future[0],\"Confirmed\"] = grouped_data.iloc[:size_x_Train+1]['total_cases'].values.tolist()\n",
    "mlp.loc[:future[0],\"Deaths\"] = grouped_data.iloc[:size_x_Train+1]['Cumulative_deaths'].values.tolist()\n",
    "nan_list=[]\n",
    "nan_list.extend(repeat(None,174))\n",
    "mlp.loc[:,\"Confirmed_predicted\"] =nan_list+ mlp_pred[:,0].tolist()\n",
    "mlp.loc[:,\"Deaths_predicted\"] =nan_list+ mlp_pred2[:,0].tolist()\n",
    "mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2)\n",
    "mlp.plot(y='Confirmed',ax=axes[0],linewidth=2).set_title('Time Forecasting')\n",
    "mlp.plot(y='Confirmed_predicted',linestyle='dashed',ax=axes[0],linewidth=2)\n",
    "mlp.plot(y='Deaths',ax=axes[1],linewidth=2).set_title('Time Forecasting')\n",
    "mlp.plot(y='Deaths_predicted',linestyle='dashed',ax=axes[1],linewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MAPE of confirmed casses is \",MAPE(mlp_conf_df[\"Confirmed\"][:n_steps],mlp_conf_df[\"Confirmed_predicted\"][:n_steps])*100,\" %\")\n",
    "print(\"Accuracy for confirmed cases is: \",(1-MAPE(mlp_conf_df[\"Confirmed\"][:n_steps],mlp_conf_df[\"Confirmed_predicted\"][:n_steps]))*100,\"\\n\")\n",
    "print(\"MAPE of deaths is \",MAPE(mlp_deaths_df[\"Deaths\"][:n_steps],mlp_deaths_df[\"Deaths_predicted\"][:n_steps]), \" %\")\n",
    "print(\"Accuracy for deaths is: \",(1-MAPE(mlp_deaths_df[\"Deaths\"][:n_steps],mlp_deaths_df[\"Deaths_predicted\"][:n_steps]))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Training SVR and predicting***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scalerX1 = MinMaxScaler()\n",
    "scalerY1 = MinMaxScaler()\n",
    "scalerX2 = MinMaxScaler()\n",
    "scalerY2 = MinMaxScaler()\n",
    "svr_x_train1 = scalerX1.fit_transform(np.array(asiaE_x_train).reshape(-1,1))\n",
    "svr_y_train1 = scalerY1.fit_transform(np.array(asiaE_y_train).reshape(-1,1))\n",
    "svr_x_test1 = scalerX1.transform(np.array(asiaE_x_test).reshape(-1,1))\n",
    "svr_y_test1 = scalerY1.transform(np.array(asiaE_y_test).reshape(-1,1))\n",
    "svr_x_train2 = scalerX2.fit_transform(np.array(asiaE_x_train2).reshape(-1,1))\n",
    "svr_z_train = scalerY2.fit_transform(np.array(asiaE_z_train).reshape(-1,1))\n",
    "svr_x_test2 = scalerX2.transform(np.array(asiaE_x_test2).reshape(-1,1))\n",
    "svr_z_test = scalerY2.transform(np.array(asiaE_z_test).reshape(-1,1))\n",
    "svm=SVR(kernel=\"rbf\", C=10,epsilon=0.001,gamma='scale')\n",
    "svm.fit(svr_x_train1,svr_y_train1)\n",
    "svr_asia_pred1=scalerY1.inverse_transform(np.array(svm.predict(svr_x_test1)).reshape(-1,1))\n",
    "svr_asia_pred1=np.array(svr_asia_pred1).reshape(-1)\n",
    "svm.fit(svr_x_train2,svr_z_train)\n",
    "svr_asia_pred2=scalerY2.inverse_transform(np.array(svm.predict(svr_x_test2)).reshape(-1,1))\n",
    "svr_asia_pred2=np.array(svr_asia_pred2).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Plot SVR Model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(1, 2)\n",
    "sns.scatterplot(asiaE_x_test,asiaE_y_test,color='#A7A7A7',s=75,label=\"Actual\",ax=axes[0])\n",
    "sns.lineplot(asiaE_x_test,svr_asia_pred1,color=\"#A4D54E\",linewidth=2.25,label=\"Predicted\",ax=axes[0])\n",
    "sns.scatterplot(asiaE_x_test2,asiaE_z_test,color='#A7A7A7',s=75,label=\"Actual\",ax=axes[1])\n",
    "sns.lineplot(asiaE_x_test2,np.array(svr_asia_pred2).reshape(-1),color=\"#F94350\",linewidth=2.25,label=\"Predicted\",ax=axes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Mean absolute percentage error***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MAPE of confirmed casses is \",MAPE(asiaE_y_test,svr_asia_pred1)*100, \" %\")\n",
    "print(\"Accuracy for confirmed cases is: \",(1-MAPE(asiaE_y_test,svr_asia_pred1))*100,\"\\n\")\n",
    "print(\"MAPE of deaths is \",MAPE(asiaE_z_test,svr_asia_pred2)*100, \" %\")\n",
    "print(\"Accuracy for deaths is: \",(1-MAPE(asiaE_z_test,svr_asia_pred2))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***R Square***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"R square score for total cases\",r2_score(asiaE_y_test, svr_asia_pred1)*100)\n",
    "print(\"R square score for total cases\",r2_score(asiaE_z_test, svr_asia_pred2)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Cross Validation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_scores1 = cross_val_score(svm, np.array(svr_x_train1).reshape(-1,1), np.array(svr_y_train1).reshape(-1,1), scoring='r2', cv=5)\n",
    "print(\"Cross Validation scores for total cases :\",svr_scores1)\n",
    "print(\"Mean of cross validation scores for total cases= \",np.mean(svr_scores1),'\\n')\n",
    "svr_scores2 = cross_val_score(svm, np.array(svr_x_train1).reshape(-1,1), np.array(svr_y_train1).reshape(-1,1), scoring='r2', cv=5)\n",
    "print(\"Cross Validation scores for cumulative deaths :\",svr_scores2)\n",
    "print(\"Mean of cross validation scores for cumulative deaths= \",np.mean(svr_scores2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference:\n",
    "This is a very import result.\n",
    "1. If we exclude India from our Asian dataset the overall data follows a linear nature.\n",
    "2. Due to this linear nature, linear regression's accuracy is very high\n",
    "3. India was the worst hit COVID country in Asia\n",
    "4. For this particular trend, SVR kernel should be linear\n",
    "5. Both SVR and linear regression perform good"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
